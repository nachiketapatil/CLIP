**CLIP Model from Scratch (Flickr30k)**

This repository contains the implementation of the CLIP (Contrastive Language-Image Pretraining) model, which was trained from scratch using the Flickr30k dataset. The model is capable of understanding the relationship between images and their corresponding textual descriptions, enabling two core functionalities:

1) Image Generation from Caption: Given an input text caption, the model generates a relevant image based on the learned relationship between the textual and visual modalities.

2) Caption Generation from Image: Given an input image, the model generates a description that best matches the content of the image.

This project is done for **FML(CS725)** at **IIT Bombay**

**Team Members**:
* Abhishek
* Nasir
* Utkarsh
* Nachiketa

**Link to PPT**:
[PPT](https://docs.google.com/presentation/d/1pLBjGjnPWlIFFBw2ThleTIKHX2gpTOmtNSQgSYOytOg/edit?usp=sharing)

**Link to CLIP repository and code**:
[CLIP Github](https://github.com/openai/CLIP)

**Link to paper**:
[CLIP Paper](https://arxiv.org/pdf/2103.00020)

**Link to Dataset**:
[Flickr30k Dataset](https://shannon.cs.illinois.edu/DenotationGraph/)
