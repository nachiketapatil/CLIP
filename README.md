# CLIP
This repository implements a CLIP Model, trained on the Flickr30k dataset. It enables two main functionalities: 1) Image Generation from Caption: generating relevant images based on text descriptions, and 2) Caption Generation from Image: generating descriptive captions for input images, by learning cross-modal relationships
